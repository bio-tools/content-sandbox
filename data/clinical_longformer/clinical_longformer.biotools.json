{
    "accessibility": "Open access",
    "additionDate": "2023-02-13T19:31:10.509233Z",
    "biotoolsCURIE": "biotools:clinical_longformer",
    "biotoolsID": "clinical_longformer",
    "confidence_flag": "tool",
    "cost": "Free of charge",
    "credit": [
        {
            "email": "yuan.luo@northwestern.edu",
            "name": "Yuan Luo",
            "typeEntity": "Person"
        }
    ],
    "description": "Clinical-Longformer is a clinical knowledge enriched version of Longformer that was further pre-trained using MIMIC-III clinical notes.",
    "editPermission": {
        "type": "public"
    },
    "function": [
        {
            "operation": [
                {
                    "term": "Editing",
                    "uri": "http://edamontology.org/operation_3096"
                },
                {
                    "term": "Named-entity and concept recognition",
                    "uri": "http://edamontology.org/operation_3280"
                }
            ]
        }
    ],
    "homepage": "https://github.com/luoyuanlab/Clinical-Longformer",
    "language": [
        "Python"
    ],
    "lastUpdate": "2023-02-13T19:31:10.511930Z",
    "license": "MIT",
    "link": [
        {
            "type": [
                "Other"
            ],
            "url": "https://huggingface.co/yikuan8/Clinical-Longformer"
        }
    ],
    "name": "Clinical-Longformer",
    "owner": "Chan019",
    "publication": [
        {
            "doi": "10.1093/JAMIA/OCAC225",
            "metadata": {
                "abstract": "Â© The Author(s) 2022. Published by Oxford University Press on behalf of the American Medical Informatics Association. All rights reserved. For permissions, please email: journals.permissions@oup.com.OBJECTIVE: Clinical knowledge-enriched transformer models (eg, ClinicalBERT) have state-of-the-art results on clinical natural language processing (NLP) tasks. One of the core limitations of these transformer models is the substantial memory consumption due to their full self-attention mechanism, which leads to the performance degradation in long clinical texts. To overcome this, we propose to leverage long-sequence transformer models (eg, Longformer and BigBird), which extend the maximum input sequence length from 512 to 4096, to enhance the ability to model long-term dependencies in long clinical texts. MATERIALS AND METHODS: Inspired by the success of long-sequence transformer models and the fact that clinical notes are mostly long, we introduce 2 domain-enriched language models, Clinical-Longformer and Clinical-BigBird, which are pretrained on a large-scale clinical corpus. We evaluate both language models using 10 baseline tasks including named entity recognition, question answering, natural language inference, and document classification tasks. RESULTS: The results demonstrate that Clinical-Longformer and Clinical-BigBird consistently and significantly outperform ClinicalBERT and other short-sequence transformers in all 10 downstream tasks and achieve new state-of-the-art results. DISCUSSION: Our pretrained language models provide the bedrock for clinical NLP using long texts. We have made our source code available at https://github.com/luoyuanlab/Clinical-Longformer, and the pretrained models available for public download at: https://huggingface.co/yikuan8/Clinical-Longformer. CONCLUSION: This study demonstrates that clinical knowledge-enriched long-sequence transformers are able to learn long-term dependencies in long clinical text. Our methods can also inspire the development of other domain-enriched long-sequence transformers.",
                "authors": [
                    {
                        "name": "Ahmad F.S."
                    },
                    {
                        "name": "Li Y."
                    },
                    {
                        "name": "Luo Y."
                    },
                    {
                        "name": "Wang H."
                    },
                    {
                        "name": "Wehbe R.M."
                    }
                ],
                "date": "2023-01-18T00:00:00Z",
                "journal": "Journal of the American Medical Informatics Association : JAMIA",
                "title": "A comparative study of pretrained language models for long clinical text"
            },
            "pmcid": "PMC9846675",
            "pmid": "36451266"
        }
    ],
    "toolType": [
        "Script"
    ],
    "topic": [
        {
            "term": "Machine learning",
            "uri": "http://edamontology.org/topic_3474"
        },
        {
            "term": "Medical imaging",
            "uri": "http://edamontology.org/topic_3384"
        },
        {
            "term": "Natural language processing",
            "uri": "http://edamontology.org/topic_0218"
        },
        {
            "term": "Preclinical and clinical studies",
            "uri": "http://edamontology.org/topic_3379"
        }
    ]
}
