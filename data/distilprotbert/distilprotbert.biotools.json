{
    "accessibility": "Open access",
    "additionDate": "2023-02-26T22:30:58.147285Z",
    "biotoolsCURIE": "biotools:distilprotbert",
    "biotoolsID": "distilprotbert",
    "confidence_flag": "tool",
    "cost": "Free of charge",
    "credit": [
        {
            "email": "yaron.geffen@biu.ac.il",
            "name": "Yaron Geffen",
            "typeEntity": "Person"
        },
        {
            "name": "Yanay Ofran"
        },
        {
            "name": "Ron Unger",
            "orcidid": "http://orcid.org/0000-0003-4153-3922"
        }
    ],
    "description": "A distilled protein language model used to distinguish between real proteins and their randomly shuffled counterparts.",
    "editPermission": {
        "type": "private"
    },
    "function": [
        {
            "operation": [
                {
                    "term": "Protein modelling",
                    "uri": "http://edamontology.org/operation_0477"
                },
                {
                    "term": "Protein secondary structure prediction",
                    "uri": "http://edamontology.org/operation_0267"
                },
                {
                    "term": "Protein structure validation",
                    "uri": "http://edamontology.org/operation_0321"
                }
            ]
        }
    ],
    "homepage": "https://github.com/yarongef/DistilProtBert",
    "language": [
        "Python"
    ],
    "lastUpdate": "2023-02-26T22:30:58.149837Z",
    "license": "MIT",
    "name": "DistilProtBert",
    "operatingSystem": [
        "Linux",
        "Mac",
        "Windows"
    ],
    "owner": "Jennifer",
    "publication": [
        {
            "doi": "10.1093/bioinformatics/btac474",
            "metadata": {
                "abstract": "Â© The Author(s) 2022. Published by Oxford University Press. All rights reserved. For permissions, please e-mail: journals.permissions@oup.com.SUMMARY: Recently, deep learning models, initially developed in the field of natural language processing (NLP), were applied successfully to analyze protein sequences. A major drawback of these models is their size in terms of the number of parameters needed to be fitted and the amount of computational resources they require. Recently, 'distilled' models using the concept of student and teacher networks have been widely used in NLP. Here, we adapted this concept to the problem of protein sequence analysis, by developing DistilProtBert, a distilled version of the successful ProtBert model. Implementing this approach, we reduced the size of the network and the running time by 50%, and the computational resources needed for pretraining by 98% relative to ProtBert model. Using two published tasks, we showed that the performance of the distilled model approaches that of the full model. We next tested the ability of DistilProtBert to distinguish between real and random protein sequences. The task is highly challenging if the composition is maintained on the level of singlet, doublet and triplet amino acids. Indeed, traditional machine-learning algorithms have difficulties with this task. Here, we show that DistilProtBert preforms very well on singlet, doublet and even triplet-shuffled versions of the human proteome, with AUC of 0.92, 0.91 and 0.87, respectively. Finally, we suggest that by examining the small number of false-positive classifications (i.e. shuffled sequences classified as proteins by DistilProtBert), we may be able to identify de novo potential natural-like proteins based on random shuffling of amino acid sequences. AVAILABILITY AND IMPLEMENTATION: https://github.com/yarongef/DistilProtBert.",
                "authors": [
                    {
                        "name": "Geffen Y."
                    },
                    {
                        "name": "Ofran Y."
                    },
                    {
                        "name": "Unger R."
                    }
                ],
                "date": "2022-09-16T00:00:00Z",
                "journal": "Bioinformatics (Oxford, England)",
                "title": "DistilProtBert: a distilled protein language model used to distinguish between real proteins and their randomly shuffled counterparts"
            },
            "pmid": "36124789"
        }
    ],
    "toolType": [
        "Script"
    ],
    "topic": [
        {
            "term": "Machine learning",
            "uri": "http://edamontology.org/topic_3474"
        },
        {
            "term": "Natural language processing",
            "uri": "http://edamontology.org/topic_0218"
        },
        {
            "term": "Proteomics",
            "uri": "http://edamontology.org/topic_0121"
        },
        {
            "term": "Sequence analysis",
            "uri": "http://edamontology.org/topic_0080"
        },
        {
            "term": "Small molecules",
            "uri": "http://edamontology.org/topic_0154"
        }
    ]
}
